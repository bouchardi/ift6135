{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabellebouchard/ift6135/blob/master/Problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VH4lIYcO1K6Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 2\n",
        "\n",
        "**Instructions**: For this part of the assignment we will train a convolutional network on MNIST\n",
        "for 10 epochs using your favorite deep learning frameworks such as Pytorch of Tensor\n",
        "ow. Plot the\n",
        "train and valid errors at the end of each epoch for the model.\n",
        "1. Come up with a CNN architecture with more or less similar number of parameters as MLP\n",
        "trained in Problem 1 and describe it.\n",
        "\n",
        "MLP in Problem 1:\n",
        "\n",
        "*   two hidden layers\n",
        "*   input data size is 784 and output is parameterized by a softmax of 10 classes\n",
        "*   train using probability loss (cross entropy) and minimize this criterion to optimize the model parameters using SGD\n",
        "*   non-linearity chosen as neuron activation (activation function): ?\n",
        "*   learning rate: ?\n",
        "*   mini-batch size: ?\n",
        "\n",
        "\n",
        "2. Compare the performances of CNN vs MLP. Comment.\n",
        "\n",
        "You could take reference from the architecture mentioned here: \n",
        "https://github.com/MaximumEntropy/welcome_tutorials/tree/pytorch/pytorch \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WesbPV2R1WnZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CRsuMTDdEhJN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzYy6Fe31sCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN vs. MLP \n",
        "\n",
        "## Number of parameters\n",
        "\n",
        "**CNN**: [x] number of parameters \n",
        "\n",
        "**MLP with 2 hidden layers**: 935,936 number of parameters\n",
        "\n",
        "input to h1 = 784 x 512 = 401,408\n",
        "\n",
        "h1 to h2 = 512 x 1024 = 524,288\n",
        "\n",
        "h2 to output = 1024 x 10 = 10,240\n",
        "\n",
        "CNN's need significantly less parameters than MLP since they exhibit sparse connectivity and parameter sharing. \n",
        "\n",
        "**Sparse connectivity**: direct connections very sparse, but in the deeper layers, units are indirectly connected to all/most of the input image. \n",
        "\n",
        "**Parameter sharing**: each member of the kernel is used at every position of the input, meaning the same parameters are used at all input locations. Therefore you learn only one set of parameters. \n",
        "\n",
        "Since we are asked to build a CNN with similar number of parameters as the MLP trained in Problem 1, the CNN should outperform the MLP since it needs a significantly smaller number of weights to be able to match an MLP's performance. \n",
        "\n",
        "## Performance\n",
        "\n",
        "If m represents the size of the input image, and n the size of the output image, then MLP requires mxn parameters with O(mxn) runtime. In CNNs, you limit the number of connections of each output to k, this means you only need kxn parameters with O(kxn) runtime. \n"
      ]
    },
    {
      "metadata": {
        "id": "U3gbXk-i0-uH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "List of references we can check out to increase our performance:\n",
        "\n",
        "*   MNIST Image Class Tensorflow CNN 99.51% Test Accur.: https://www.kaggle.com/raoulma/mnist-image-class-tensorflow-cnn-99-51-test-acc\n",
        "*   How to Choose CNN Architecture for MNIST: https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist\n",
        "*   99.75% Accur: https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist/notebook\n",
        "\n"
      ]
    }
  ]
}