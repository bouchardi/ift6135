{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem_2v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabellebouchard/ift6135/blob/master/Problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vOsrK0HhsHwK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#TODO\n",
        "- check that mnist data isn't in order from 1-9\n",
        "- I did train 80%, validate 20% (check MLP)\n",
        "- going to have to change the activation function in MLP from sigmoid (11% accuracy aka random) to ReLU (95% accuracy)\n",
        "- try using Leaky ReLU as activation function \n",
        "- can we use dropout?\n",
        "- add dilation?\n",
        "- check if its better to have smaller filters at the beginning or end of network (see textbook)\n",
        "- can we add batchnorm? \n",
        "- add more parameters to match MLP\n",
        "- separate train from evaluate (commented out right now)\n",
        "- can we change batch size? batch size 34 gives 98%\n"
      ]
    },
    {
      "metadata": {
        "id": "VH4lIYcO1K6Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 2\n",
        "\n",
        "**Instructions**: For this part of the assignment we will train a convolutional network on MNIST\n",
        "for 10 epochs using your favorite deep learning frameworks such as Pytorch of Tensor\n",
        "ow. Plot the\n",
        "train and valid errors at the end of each epoch for the model.\n",
        "1. Come up with a CNN architecture with more or less similar number of parameters as MLP\n",
        "trained in Problem 1 and describe it.\n",
        "\n",
        "MLP in Problem 1:\n",
        "\n",
        "*   two hidden layers\n",
        "*   input data size is 784 and output is parameterized by a softmax of 10 classes\n",
        "*   train using probability loss (cross entropy) and minimize this criterion to optimize the model parameters using SGD\n",
        "*   non-linearity chosen as neuron activation (activation function): sigmoid (we should try ReLu)\n",
        "*   learning rate: 1.e-2\n",
        "*   mini-batch size: 128\n",
        "\n",
        "\n",
        "2. Compare the performances of CNN vs MLP. Comment.\n",
        "\n",
        "You could take reference from the architecture mentioned here: \n",
        "https://github.com/MaximumEntropy/welcome_tutorials/tree/pytorch/pytorch \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WesbPV2R1WnZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EWHeQer4mskR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Input Image**: 28 x 28\n",
        "\n",
        "**Convolution 1**: 32 filters, kernel size = 5, stide = 1,  padding = 2\n",
        "\n",
        "**Max pooling 1**: kernel size = 2\n",
        "\n",
        "**Convolution 2**: 64 filters, kernel size = 5, stide = 1,  padding = 2\n",
        "\n",
        "**Max pooling 2**: kernel size = 2\n",
        "\n",
        "**Convolution 3**: 128 filters, kernel size = 5, stide = 1,  padding = 2\n",
        "\n",
        "**Max pooling 3**: kernel size = 2\n",
        "\n",
        "**Convolution 4**: 246 filters, kernel size = 3, stide = 1,  padding = 2\n",
        "\n",
        "**Max pooling 4**: kernel size = 2\n",
        "\n",
        "**Fully Connected Layer**: 2 x 2 x 246 \n",
        "\n",
        "**Ouput:** 10 x 1\n",
        "\n",
        "**Loss:** softmax + cross-entropy\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "684DW9LYqBOQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Installation Requirements"
      ]
    },
    {
      "metadata": {
        "id": "bpLwFCA2qZ3Z",
        "colab_type": "code",
        "outputId": "84e086e6-5dd3-48c6-a349-59f6bc8833f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision matplotlib "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6XE-zREuubCO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use GPU \n",
        "\n",
        "First, select \"GPU\" in the Edit Menu -> Notebook Settings -> Hardware Accelerator -> GPU\n",
        "\n",
        "**torch.cuda**: *This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.*\n",
        "\n",
        "**torch.cuda.is_available()**: *Returns a bool indicating if CUDA is currently available.*\n",
        "\n",
        "To go from a tensor type CPU to GPU, add .to(\"cuda:0\")"
      ]
    },
    {
      "metadata": {
        "id": "CazsaWTvwY4a",
        "colab_type": "code",
        "outputId": "477f245e-bfff-4032-8607-19677fec8e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "print(\"GPU Available: {}\".format(use_gpu))\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d-ds7UbtqNVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MNIST Data\n",
        "\n",
        "Get MNIST Data, split in train/validate/test and load in DataLoader"
      ]
    },
    {
      "metadata": {
        "id": "CRsuMTDdEhJN",
        "colab_type": "code",
        "outputId": "336cf9fa-43b2-44de-c269-29fdea17e0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import sampler, DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "batch_size = 34\n",
        "\n",
        "# ChunkSampler class is from https://github.com/pytorch/vision/issues/168\n",
        "class ChunkSampler(sampler.Sampler):\n",
        "    \"\"\"Samples elements sequentially from some offset. \n",
        "    Arguments:\n",
        "        num_samples: # of desired datapoints\n",
        "        start: offset where we should start selecting from\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples, start = 0):\n",
        "        self.num_samples = num_samples\n",
        "        self.start = start\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(range(self.start, self.start + self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "train_set = MNIST(root='../data',train=True,transform=transforms.ToTensor(),download=True)\n",
        "test_set = MNIST(root='../data',train=False,transform=transforms.ToTensor(),download=True)\n",
        "\n",
        "train_set_size = len(train_set)\n",
        "NUM_TRAIN = int(0.8 * train_set_size) #cast to int to avoid TypeError: 'float' object cannot be interpreted as an integer\n",
        "NUM_VAL = train_set_size - NUM_TRAIN\n",
        "NUM_TEST = len(test_set)\n",
        "\n",
        "loader_train = DataLoader(train_set, batch_size=batch_size, sampler=ChunkSampler(NUM_TRAIN, 0),shuffle=False)\n",
        "loader_val = DataLoader(train_set, batch_size=batch_size, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN),shuffle=False)\n",
        "loader_test = DataLoader(test_set, batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "19H5ZWImwdHt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN Model\n",
        "\n",
        "[torch.nn.Module](https://pytorch.org/docs/master/nn.html#torch.nn.Module): Base class for all NN modules. \n",
        "Must implement __init__ (defines the layers) and **forward** (returns the output)\n",
        "\n",
        "[torch.nn.Sequential(*args)](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential): Modules will be added to it in the order they are passed in the constructor\n",
        "\n",
        " [torch.nn.Conv2d](https://pytorch.org/docs/master/nn.html#torch.nn.Conv2d)(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
        " \n",
        "[ torch.nn.Sigmoid](https://pytorch.org/docs/master/nn.html#sigmoid)\n",
        "\n",
        "[torch.nn.MaxPool2d](https://pytorch.org/docs/master/nn.html#torch.nn.MaxPool2d)(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False): Applies a 2D max pooling over an input signal composed of several input planes.\n",
        "\n",
        " [torch.nn.Dropout2d](https://pytorch.org/docs/master/nn.html#dropout2d)(p=0.5, inplace=False): *Each channel will be zeroed out independently on every forward call. with probability p using samples from a Bernoulli distribution*\n",
        "\n",
        " [torch.nn.Linear](https://pytorch.org/docs/master/nn.html#torch.nn.Linear)(in_features, out_features, bias=True)\n"
      ]
    },
    {
      "metadata": {
        "id": "BsuLybW3wyq2",
        "colab_type": "code",
        "outputId": "003d609b-01f3-47f7-df21-40882c20c26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(1,32,kernel_size=5,padding=2),\n",
        "        #nn.Sigmoid(),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2))\n",
        "    \n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(32,64,kernel_size=5,padding=2),\n",
        "        #nn.Sigmoid(),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2))\n",
        "\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(64,96,kernel_size=5,padding=2),\n",
        "        #nn.Sigmoid(),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2))\n",
        "    \n",
        "    self.conv4 = nn.Sequential(\n",
        "        nn.Conv2d(96,246,kernel_size=3,padding=2),\n",
        "        #nn.Sigmoid(),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2))\n",
        "    \n",
        "    self.conv5 = nn.Sequential(\n",
        "        nn.Conv2d(246,246,kernel_size=3,padding=2),\n",
        "        #nn.Sigmoid(),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2))\n",
        "    \n",
        "    \n",
        "    self.fc = nn.Linear(2*2*246,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.conv2(out)\n",
        "    out = self.conv3(out)\n",
        "    out = self.conv4(out)\n",
        "    out = self.conv5(out)\n",
        "    flatten = out.view(out.size(0),-1)\n",
        "    fc = self.fc(flatten)\n",
        "    return fc\n",
        "\n",
        "model = CNN()\n",
        "# put model on GPU\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "# Save the initial weights of model\n",
        "init_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(64, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(96, 246, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv5): Sequential(\n",
            "    (0): Conv2d(246, 246, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=984, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "# Parameters:  973322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "av0WomNHWIpa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train CNN\n",
        "\n",
        "#### Loss function\n",
        "\n",
        "criterion = [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss)(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
        "\n",
        "#### Minimize loss function using stochastic gradient descent\n",
        "optimizer = [torch.optim.SGD](https://pytorch.org/docs/master/optim.html#torch.optim.SGD)(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n"
      ]
    },
    {
      "metadata": {
        "id": "qNbuxqGNWPGs",
        "colab_type": "code",
        "outputId": "70bc61ef-ed83-405b-9050-ee91dd54ea1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "num_epochs = 10\n",
        "\n",
        "#loss function\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.load_state_dict(init_model_wts)\n",
        "\n",
        "train_errors = []\n",
        "eval_errors = []\n",
        "\n",
        "print(\"Training begins...\")\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  train_error = 0\n",
        "  train_num = 0\n",
        "  \n",
        "  # model in train mode\n",
        "  model.train()\n",
        "  \n",
        "  for digits, labels in loader_train:\n",
        "    \n",
        "    # digits and labels on GPU\n",
        "    digits = digits.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    # zero gradient buffer\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # forward pass\n",
        "    outputs = model(digits)\n",
        "        \n",
        "    # loss function\n",
        "    loss = criterion(outputs,labels)\n",
        "        \n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # gradient descent step\n",
        "    optimizer.step()\n",
        "    \n",
        "    # add the loss\n",
        "    train_error += loss.item()\n",
        "    train_num += 1\n",
        "  \n",
        "  eval_error = 0\n",
        "  eval_num = 0\n",
        "  \n",
        "  # model in eval mode\n",
        "  model.eval()\n",
        "  \n",
        "  for digits, labels in loader_val:\n",
        "    \n",
        "    # digits and labels on GPU\n",
        "    digits = digits.to(device)\n",
        "    labels = labels.to(device)\n",
        "        \n",
        "    # forward pass\n",
        "    outputs = model(digits)\n",
        "    \n",
        "    # loss function\n",
        "    loss = criterion(outputs,labels)\n",
        "        \n",
        "    # add the loss\n",
        "    eval_error += loss.item()\n",
        "    eval_num += 1\n",
        "  \n",
        "  # save the loss\n",
        "  train_errors.append(train_error / train_num)\n",
        "  eval_errors.append(eval_error / eval_num)\n",
        "  \n",
        "  #print eval_error after each epoch\n",
        "  print('\\nEpoch {}'.format(epoch + 1))\n",
        "  print('\\tTrain error: {:.4f}'.format(train_error/train_num))  \n",
        "  print('\\tEval error: {:.4f}'.format(eval_error/eval_num))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training begins...\n",
            "\n",
            "Epoch 1\n",
            "\tTrain error: 2.2667\n",
            "\tEval error: 1.8317\n",
            "\n",
            "Epoch 2\n",
            "\tTrain error: 0.5563\n",
            "\tEval error: 0.2318\n",
            "\n",
            "Epoch 3\n",
            "\tTrain error: 0.1675\n",
            "\tEval error: 0.1345\n",
            "\n",
            "Epoch 4\n",
            "\tTrain error: 0.1096\n",
            "\tEval error: 0.1058\n",
            "\n",
            "Epoch 5\n",
            "\tTrain error: 0.0843\n",
            "\tEval error: 0.0929\n",
            "\n",
            "Epoch 6\n",
            "\tTrain error: 0.0692\n",
            "\tEval error: 0.0857\n",
            "\n",
            "Epoch 7\n",
            "\tTrain error: 0.0589\n",
            "\tEval error: 0.0799\n",
            "\n",
            "Epoch 8\n",
            "\tTrain error: 0.0510\n",
            "\tEval error: 0.0739\n",
            "\n",
            "Epoch 9\n",
            "\tTrain error: 0.0447\n",
            "\tEval error: 0.0685\n",
            "\n",
            "Epoch 10\n",
            "\tTrain error: 0.0395\n",
            "\tEval error: 0.0656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-CaHug8ywF-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate CNN\n"
      ]
    },
    {
      "metadata": {
        "id": "fa8RYeR9wOmb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# eval_errors = []\n",
        "\n",
        "# print(\"Evaluation begins...\")\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "  \n",
        "#   eval_error = 0\n",
        "#   eval_num = 0\n",
        "  \n",
        "#   # model in eval mode\n",
        "#   model.eval()\n",
        "  \n",
        "#   for digits, labels in loader_val:\n",
        "    \n",
        "#     # digits and labels on GPU\n",
        "#     digits = digits.to(device)\n",
        "#     labels = labels.to(device)\n",
        "        \n",
        "#     # forward pass\n",
        "#     outputs = model(digits)\n",
        "    \n",
        "#     # loss function\n",
        "#     loss = criterion(outputs,labels)\n",
        "        \n",
        "#     # add the loss\n",
        "#     eval_error += loss.item()\n",
        "#     eval_num += 1\n",
        "  \n",
        "#   # save the loss\n",
        "#   eval_errors.append(eval_error / eval_num)\n",
        "  \n",
        "#   #print eval_error after each epoch\n",
        "#   print('\\nEpoch {}'.format(epoch + 1))\n",
        "#   print('\\tEval error: {:.4f}'.format(eval_error/eval_num))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jsv2qq5dxdXK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot Train and Valid Errors"
      ]
    },
    {
      "metadata": {
        "id": "ppiHJuZTxl89",
        "colab_type": "code",
        "outputId": "9b6ea0e7-3e89-4b23-9ff0-0e1073edc7ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = list(range(len(train_errors))) \n",
        "plt.plot(x, train_errors,'m',label='Train')\n",
        "plt.plot(x, eval_errors,'g', label='Validation')\n",
        "\n",
        "plt.xlabel('Number Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best',shadow=True, fancybox=True)\n",
        "plt.title(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5CgFgIhIjvr/kO6Bg0TTWYkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "20iwVyynyVgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test CNN"
      ]
    },
    {
      "metadata": {
        "id": "NMzvWhFxyXhl",
        "colab_type": "code",
        "outputId": "40e13fc5-98af-4d67-faf6-9df776867381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# model in eval mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for digits, labels in loader_test:\n",
        "  \n",
        "  # digits and labels on GPU\n",
        "  digits = digits.to(device)\n",
        "  labels = labels.to(device)\n",
        "  \n",
        "  # forward pass\n",
        "  outputs = model(digits)\n",
        "  _, predicted = torch.max(outputs.data, 1)\n",
        "  \n",
        "  # save the accuary\n",
        "  \n",
        "  total += labels.size(0)\n",
        "  correct += torch.sum(predicted == labels.data)\n",
        "print (correct)\n",
        "print (total)\n",
        "#print (100 * correct / total)  \n",
        "print('Accuracy on the test set: {:.4f}%'.format(100 * correct / total))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(9826, device='cuda:0')\n",
            "10000\n",
            "Accuracy on the test set: 98.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TzYy6Fe31sCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN vs. MLP \n",
        "\n",
        "## Number of parameters\n",
        "\n",
        "**CNN with 4 conv layers**\n",
        "\n",
        "conv1 = 32 filters size 5x5 + 32 biases = 32x5x5+32 = 832\n",
        "\n",
        "conv2 = 32x64 filters size 5x5 + 64 biases = 32x64x5x5 + 64 = 51,264\n",
        "\n",
        "conv3 = 64x96 filters size 5x5 + 96 biases = 64x96x5x5 + 128 = 153696\n",
        "\n",
        "conv4 = 96x246 filters size 3x3 + 246 biases = 96x246x3x3 + 246 = 212790\n",
        "\n",
        "conv5 = 246x246 filters size 3x3 + 246 biases = 246x246x3x3 + 246 = 544890\n",
        "\n",
        "fc = 1x(2x2x246) x 10 + 10 = 9850\n",
        "\n",
        "total CNN parameters = 832 + 51,264 + 153,696 +212790 + 544890 + 9850 = **973322**\n",
        "\n",
        "\n",
        "\n",
        "**MLP with 2 hidden layers**\n",
        "\n",
        "input to h1 = 784 x 512 + 512 = 401,408\n",
        "\n",
        "h1 to h2 = 512 x 1024 + 1024 = 524,288\n",
        "\n",
        "h2 to output = 1024 x 10  + 10 = 10,240\n",
        "\n",
        "total MLP parameters = 401,408 + 524,288 + 10,240 = **937482**\n",
        "\n",
        "## Discussion\n",
        "\n",
        "CNN's need significantly less parameters than MLP since they exhibit sparse connectivity and parameter sharing. \n",
        "\n",
        "**Sparse connectivity**: direct connections very sparse, but in the deeper layers, units are indirectly connected to all/most of the input image. \n",
        "\n",
        "**Parameter sharing**: each member of the kernel is used at every position of the input, meaning the same parameters are used at all input locations. Therefore you learn only one set of parameters. \n",
        "\n",
        "Since we are asked to build a CNN with similar number of parameters as the MLP trained in Problem 1, the CNN should outperform the MLP since it needs a significantly smaller number of weights to be able to match an MLP's performance. \n",
        "\n",
        "## Performance\n",
        "\n",
        "If m represents the size of the input image, and n the size of the output image, then MLP requires mxn parameters with O(mxn) runtime. In CNNs, you limit the number of connections of each output to k, this means you only need kxn parameters with O(kxn) runtime. \n"
      ]
    },
    {
      "metadata": {
        "id": "U3gbXk-i0-uH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "List of references we can check out to increase our performance:\n",
        "\n",
        "*   MNIST Image Class Tensorflow CNN 99.51% Test Accur.: https://www.kaggle.com/raoulma/mnist-image-class-tensorflow-cnn-99-51-test-acc\n",
        "*   How to Choose CNN Architecture for MNIST: https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist\n",
        "*   99.75% Accur: https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist/notebook\n",
        "\n"
      ]
    }
  ]
}